{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4rVbuZEg8ZZ"
   },
   "source": [
    "# Capstone Part 2d: GPT-2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEeQSzqDg8Zk"
   },
   "source": [
    "Preface: Modelling was done on Colab, so there are a few lines of code that are useful only on Colab.\n",
    "\n",
    "We now try to finetune a pretrained GPT-2 model on our dataset and see if it outperforms the rest.\n",
    "\n",
    "Note: To use this notebook, run the import cells as well as the functions cells, then go straight to the 'Results' tab.\n",
    "\n",
    "Abstract: GPT, or Generative Pre-trained Transformer, is an attention model which learns to focus attention on the previous words that are most relevant to the task at hand. This is done by assigning weights through the decoder to specific states in the past, thus creating a 'context'. Since this is a transfer model, the modelling process simply requires us to fine tune the downloaded model from OpenAI.\n",
    "\n",
    "Reference: https://github.com/openai/gpt-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ck6ZfVFjg8Zm"
   },
   "source": [
    "A notice on decoding method used. There are several decoding methods in transformers (or any encoding decoding based models), and each gives quite different results.\n",
    "\n",
    "1. Greedy Search: Uses the word with the highest probability given a context as the next word. This leads to a repetition of a phrase if the phrase ends with the first word of the phrase. (**I** love my dog but **I** love my dog but **I**...)\n",
    "\n",
    "\n",
    "2. Beam Search: Depending on the number of beams x, it keeps track of x highest probability paths it has taken, and takes the highest probability path out of these x paths until the EOS token. Coupled with the n-gram penalty parameter, it ensures that a phrase of n length does not appear twice in the generated text.\n",
    "\n",
    "\n",
    "3. Top-k Sampling: The top k words are filtered and the probability mass is redistributed among only those words, which is what GPT2 defaultly uses. This expands the vocabulary of the generator considerably, but also opens it up to less than par word sequences. To combat this, the top-k parameter and temperature parameter are used. The temperature parameter determines the limit of randomness the model is willing to accept, with 0 being it only accepts the highest probability word (similar to greedy search).\n",
    "\n",
    "\n",
    "4. Nucleus Sampling: The smallest possible set of words whose cumulative probability exceeds the probability p is chosen. The probability mass is then redistributed similarly to top-k throughout this set of words. This means that the more similarly even the level of probability for the top few choices, the more choices the generator has to pick for the next word. This is the sampling method used below, and can be identified by the parameter 0 < top_p < 1.\n",
    "\n",
    "Reference: https://huggingface.co/blog/how-to-generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XgcUZ6fLEFW_"
   },
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import fire\n",
    "import logging\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# pytorch imports\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# tranformers imports\n",
    "from transformers import (GPT2Tokenizer,\n",
    "                          GPT2LMHeadModel,\n",
    "                          AdamW,\n",
    "                          get_linear_schedule_with_warmup)\n",
    "\n",
    "# misc imports\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "f7BPdJ1pEO4B"
   },
   "outputs": [],
   "source": [
    "# instantiate dataset class\n",
    "class email(Dataset):\n",
    "    \n",
    "    def __init__(self, truncate=False, gpt2_type='gpt2', max_length=768):\n",
    "        \n",
    "        # instantiate pretrained tokenizer\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "        self.emails = []\n",
    "\n",
    "        with open('enron6_clean.csv', newline='') as csvfile:\n",
    "            email_csv = csv.reader(csvfile)\n",
    "            for row in email_csv:\n",
    "                # encode text into tensors\n",
    "                self.emails.append(torch.tensor(\n",
    "                    self.tokenizer.encode(\n",
    "                        # 768 characters is gpt2-small's limit\n",
    "                        # endoftext is gpt2 specific delimiter\n",
    "                        f'{row[0][:max_length]}<|endoftext|>'\n",
    "                    )))\n",
    "                \n",
    "        if truncate:\n",
    "            self.emails = self.emails[:20000]\n",
    "            \n",
    "        self.email_count = len(self.emails)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.email_count\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.emails[item] # return a particular tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MAGXbKu5E-gc"
   },
   "outputs": [],
   "source": [
    "# ensure each input tensors have as much text as possible\n",
    "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
    "    if packed_tensor is None:\n",
    "        return new_tensor, True, None\n",
    "    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n",
    "        return packed_tensor, False, new_tensor\n",
    "    else:\n",
    "        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n",
    "        return packed_tensor, True, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6DdyjQUXFtsv"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataset,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    batch_size=16,\n",
    "    epochs=20,\n",
    "    lr=2e-5,\n",
    "    max_seq_len=400,\n",
    "    warmup_steps=5000,\n",
    "    gpt2_type='gpt2',\n",
    "    device='cuda',\n",
    "    output_dir='',\n",
    "    output_prefix='',\n",
    "    test_mode=False,\n",
    "    save_model_on_epoch=False\n",
    "):\n",
    "\n",
    "    acc_steps = 100\n",
    "\n",
    "    model = model.to(device) # set to cuda (on colab)\n",
    "    model.train() # switch into training mode\n",
    "    optimizer = AdamW(model.parameters(), lr=lr) # assign optimizer\n",
    "    \n",
    "    # initialize optimizer schedule\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
    "    ) \n",
    "\n",
    "    # initialize dataloader to iterate through dataset\n",
    "    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    accumulating_batch_count = 0\n",
    "    input_tensor = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(f'Training epoch {epoch}')\n",
    "        \n",
    "        # loop through each batch in the dataloader object\n",
    "        for idx, entry in tqdm(enumerate(train_dataloader)):\n",
    "            \n",
    "            # fill up tensor to 768 capacity\n",
    "            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)\n",
    "\n",
    "            if carry_on and idx != len(train_dataloader) - 1:\n",
    "                continue\n",
    "\n",
    "            input_tensor = input_tensor.to(device)\n",
    "            outputs = model(input_tensor, labels=input_tensor)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "\n",
    "            # only run a step if the batch is fully packed\n",
    "            if (accumulating_batch_count % batch_size) == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                model.zero_grad()\n",
    "\n",
    "            accumulating_batch_count += 1\n",
    "            input_tensor = None # refresh ram memory\n",
    "            \n",
    "        if save_model_on_epoch and epoch==10: # save model when epoch complete\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(output_dir, f'{output_prefix}-{epoch}.pt'),\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uojQxDBYEtXF"
   },
   "outputs": [],
   "source": [
    "data = email(truncate=True, gpt2_type='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xUGNsc7zF9B4",
    "outputId": "050e2f6b-e4e4-4032-8eb4-26c51da1c249"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17402it [01:30, 192.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17402it [01:30, 191.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17402it [01:30, 191.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17402it [01:30, 191.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17402it [01:30, 191.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17402it [01:30, 191.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17402it [01:31, 190.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17402it [01:30, 192.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17402it [01:30, 191.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17402it [01:30, 191.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17402it [01:30, 191.82it/s]\n"
     ]
    }
   ],
   "source": [
    "model = train(\n",
    "    data,\n",
    "    GPT2LMHeadModel.from_pretrained('gpt2'),\n",
    "    GPT2Tokenizer.from_pretrained('gpt2'),\n",
    "    batch_size=16,\n",
    "    epochs=11,\n",
    "    lr=3e-5,\n",
    "    max_seq_len=140,\n",
    "    warmup_steps=5000,\n",
    "    gpt2_type='gpt2',\n",
    "    device='cuda',\n",
    "    output_dir='trained_models',\n",
    "    output_prefix='email',\n",
    "    save_model_on_epoch=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkgiX61Bg8Zs"
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SLKedb1sg8Zt",
    "outputId": "c06dd15d-b983-4bcb-8c08-58032e969330"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.load_state_dict(torch.load('models/gpt2_10epochs.pt', map_location='cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HisjavqwF-pA"
   },
   "outputs": [],
   "source": [
    "# adapted from Huggingface's run_generation.py script\n",
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    entry_count=1,\n",
    "    entry_length=100,\n",
    "    top_p=0.8,\n",
    "    temperature=1.,\n",
    "):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    generated_num = 0\n",
    "\n",
    "    filter_value = -float('Inf')\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for entry_idx in trange(entry_count):\n",
    "\n",
    "            entry_finished = False\n",
    "\n",
    "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "\n",
    "            for i in range(entry_length):\n",
    "                outputs = model(generated, labels=generated)\n",
    "                loss, logits = outputs[:2]\n",
    "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(\n",
    "                    F.softmax(sorted_logits, dim=-1), dim=-1\n",
    "                )\n",
    "\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                    ..., :-1\n",
    "                ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = filter_value\n",
    "\n",
    "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "                generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "                if next_token in tokenizer.encode('<|endoftext|>'):\n",
    "                    entry_finished = True\n",
    "\n",
    "                if entry_finished:\n",
    "\n",
    "                    generated_num = generated_num + 1\n",
    "\n",
    "                    output_list = list(generated.squeeze().numpy())\n",
    "                    output_text = tokenizer.decode(output_list)\n",
    "\n",
    "                    break\n",
    "            \n",
    "            if not entry_finished:\n",
    "                output_list = list(generated.squeeze().numpy())\n",
    "                output_text = f'{tokenizer.decode(output_list)}<|endoftext|>' \n",
    "                \n",
    "                \n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0elcTzMvg8Zu"
   },
   "source": [
    "### Seeded Text Generation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GKJ-3ReBL12X",
    "outputId": "b86a8c0a-e742-4dc9-e17a-c5d5e808a29b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:32<00:00, 32.72s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Please give me an update on the progress of this game, we're working hard to get everything to you in as soon as possible!\\n\\nBefore getting into specifics, a good thing to know about PvP. If you don't like PvP but would like to try it out, you're welcome to get in on the game and play!\\n\\nWe've implemented a couple changes for PvP in Patch 6.0.\\n\\nNew Weapon Use Rate\\n\\nAdded the new UPROMIZE Damage class. It's a Weapon with a big<|endoftext|>\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model.to('cpu'), GPT2Tokenizer.from_pretrained('gpt2'),'Please give me an update on the progress of',entry_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "St0ZFpzzg8Zv"
   },
   "source": [
    "GPT-2 is very interesting in that it was trained in 8 million webpages, of which the subject ranges wildly. As we can see in the above generated text, as the seeded text was rather vague in terms of topic, it randomly pulled out words that made grammatical sense but did not necessarily have the correct topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tJe5nd4_Ksaa",
    "outputId": "ef82d31c-9177-4de6-a128-3bfcd2823bcd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:32<00:00, 32.51s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I require the financial reports by today. If you do not have enough information, send it to me, and I will add it to the list of possible financial reports as quickly as possible. To the extent possible, please send me the name of the person to whom the financial reports are for payment. In addition, we ask that your name be used as the subject line for the documents.\\n\\nPlease note that I am not responsible for any costs incurred in providing your personal information.<|endoftext|>']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model.to('cpu'), GPT2Tokenizer.from_pretrained('gpt2'),'I require the financial reports by today.',entry_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GTyOQW0g8Zw"
   },
   "source": [
    "However, if the topic is stated explicitly ('financial reports'), it does give a paragraph of relevant text. Unfortunately, this paragraph of text sounds awfully like a scam email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iLDR0TURg8Zw",
    "outputId": "d8c0ebb7-ca59-4b96-9599-234255cfe590"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:30<00:00, 30.49s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Enron as a company with millions of customers around the world. The company has moved to become more efficient, more agile, more responsive, more responsive. And like many IT companies, we\\'ve lost the energy and enthusiasm to run and navigate our business and which business and service can we utilize to provide an improved customer experience.\\n\\nThat\\'s why we are introducing our \"Integrated Customer Experience\" to support customers and introduce your Customer Service team as a new firm as part of our new $19 Million Company of Advisors<|endoftext|>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model.to('cpu'), GPT2Tokenizer.from_pretrained('gpt2'),'Enron as a company',entry_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gG-t0rX1g8Zw"
   },
   "source": [
    "And now it has become an advertisement of sorts. Hilariously though, it doesn't really work as an advertisement that well either ('we've lost the energy and enthusiasm to run and navigate our business', what?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b_H-MNEm9bC9",
    "outputId": "1ba4ca5d-f34c-4edd-e306-4c55f7c552e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:19<00:00, 19.34s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['There will be an auditor at 3pm, and at 5pm all kinds of other stuff will be happening.\\n\\n\"Our A.J. room will be open all day, it\\'s just going to be a lot of activity.\"\\n\\nDo you have a location to call for updates on the venue? Email scott@phillynews.com<|endoftext|>']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model.to('cpu'), GPT2Tokenizer.from_pretrained('gpt2'),'There will be an auditor at 3pm',entry_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujCU0k-Q-ppu"
   },
   "source": [
    "This, by far, the most email like text of them all! It is relevant, it picked up the topic, and it sounds like an email that would be sent in an office setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3qSdjBZg8Zx"
   },
   "source": [
    "### Topic-based Text Generation Test\n",
    "\n",
    "We've seen that with seeded text, it can pick up the topic within. However, what about just the topic as a singular word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NiKFvEr6g8Zx",
    "outputId": "676dee6f-6c51-4039-babd-6389e93c6c30"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:30<00:00, 30.49s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'finance and make sure we have the tools we need to make sure the improvements in those wallets are taken into account,\" Sandoval told CoinDesk.\\n\\nBorow said that if the wallet will be successfully run on more exchanges, he expects more exchanges to join the market and exchange users to be able to upgrade their wallets, adding that exchanges will also have an option to update their own wallets for those wallets, along with reporting affected wallets.\\n\\nThe adoption of block size reduction\\n\\nThe announcement<|endoftext|>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model.to('cpu'), GPT2Tokenizer.from_pretrained('gpt2'),'finance',entry_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "SZJyq-RMg8Zx",
    "outputId": "b22ddb11-3dc9-4833-fa16-6a89bf07c7a7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:36<00:00, 36.51s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'schedule a meeting with the government of British Columbia about its proposed \"Enhanced Action Plan for Extraordinary Suspension of Regulation.\" These provisions are likely to be watered down, but the passage of those provisions is also likely to be seen as being an effort to help the Crown not directly benefit from a development that would have killed every SPC in Canada, and that Canada would not be able to buy.\\n\\nThe increase in the costs to the economy of the agreement means that there will be a considerable impact on private-<|endoftext|>'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model.to('cpu'), GPT2Tokenizer.from_pretrained('gpt2'),'schedule a meeting',entry_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KbGwinCg8Zy"
   },
   "source": [
    "It sort of works, but you have to be really specific as any amount of ambiguity can spiral off into something completely irrelevant. Also, it can't really decide on the tonality of the text based off a single word, so it's not the best at it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEg_WTWrg8Zy"
   },
   "source": [
    "### Conclusions\n",
    "\n",
    "Advantages of GPT2:\n",
    "\n",
    "1. The best at generating coherent and human-like writing.\n",
    "\n",
    "2. Rather quick to finetune (3 minutes per epoch).\n",
    "\n",
    "3. It is rather tolerant to noise.\n",
    "\n",
    "\n",
    "Disadvantages of GPT2:\n",
    "\n",
    "1. Tends to go off at tangents at the slightest bit of ambiguity.\n",
    "\n",
    "2. It tends to write in an articular manner more than anything which might not be desirable behaviour.\n",
    "\n",
    "3. It cannot autocomplete on a word level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDiLkfvv9baW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Capstone 2e - GPT2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
