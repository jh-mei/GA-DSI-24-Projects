{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XgcUZ6fLEFW_"
   },
   "outputs": [],
   "source": [
    "# Using PyTorch 1.4\n",
    "\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import fire\n",
    "import logging\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "t22huoB0EHvH"
   },
   "outputs": [],
   "source": [
    "class ParquetDataset(Dataset):\n",
    "    def __init__(self, path, cols, truncate=False, gpt2_type=\"gpt2\", max_length=768):\n",
    "\n",
    "        # Grab our pandas dataframe, only reading in the columns we're interested in,\n",
    "        # append our magic tokens (<#col_name#> for the particular column, and <|endoftext|>\n",
    "        # used by GPT-2 as a text separator), then concatenate them into one giant column for\n",
    "        # our dataset\n",
    "\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "        \n",
    "        self.df = pq.read_table(path, columns=cols).to_pandas().dropna()\n",
    "        for col in cols:\n",
    "            self.df[col] = self.df[col].apply(lambda x: torch.tensor(self.tokenizer.encode(f\"<#{col}#>{x[:768]}<|endoftext|>\")))\n",
    "        self.df = pd.concat(map(self.df.get, cols)).reset_index(drop=True)\n",
    "        if truncate:\n",
    "            self.df = self.df.truncate(after=150)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.count()\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.df.iloc[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "f7BPdJ1pEO4B"
   },
   "outputs": [],
   "source": [
    "class email(Dataset):\n",
    "    \n",
    "    def __init__(self, control_code, truncate=False, gpt2_type=\"gpt2\", max_length=768):\n",
    "\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "        self.emails = []\n",
    "\n",
    "        with open('enron6_clean.csv', newline='') as csvfile:\n",
    "            email_csv = csv.reader(csvfile)\n",
    "            for row in email_csv:\n",
    "                self.emails.append(torch.tensor(\n",
    "                    self.tokenizer.encode(f\"<|{control_code}|>{row[0][:max_length]}<|endoftext|>\")\n",
    "                ))\n",
    "                \n",
    "        if truncate:\n",
    "            self.emails = self.emails[:20000]\n",
    "        self.email_count = len(self.emails)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.email_count\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.emails[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MAGXbKu5E-gc"
   },
   "outputs": [],
   "source": [
    "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
    "    if packed_tensor is None:\n",
    "        return new_tensor, True, None\n",
    "    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n",
    "        return packed_tensor, False, new_tensor\n",
    "    else:\n",
    "        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n",
    "        return packed_tensor, True, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6DdyjQUXFtsv"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataset,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    batch_size=16,\n",
    "    epochs=4,\n",
    "    lr=2e-5,\n",
    "    max_seq_len=400,\n",
    "    warmup_steps=5000,\n",
    "    gpt2_type=\"gpt2\",\n",
    "    device=\"cuda\",\n",
    "    output_dir=\".\",\n",
    "    output_prefix=\"wreckgar\",\n",
    "    test_mode=False,\n",
    "    save_model_on_epoch=False,\n",
    "):\n",
    "\n",
    "    acc_steps = 100\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    accumulating_batch_count = 0\n",
    "    input_tensor = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(f\"Training epoch {epoch}\")\n",
    "        for idx, entry in tqdm(enumerate(train_dataloader)):\n",
    "            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)\n",
    "\n",
    "            if carry_on and idx != len(train_dataloader) - 1:\n",
    "                continue\n",
    "\n",
    "            input_tensor = input_tensor.to(device)\n",
    "            outputs = model(input_tensor, labels=input_tensor)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "\n",
    "            if (accumulating_batch_count % batch_size) == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                model.zero_grad()\n",
    "\n",
    "            accumulating_batch_count += 1\n",
    "            input_tensor = None\n",
    "        if save_model_on_epoch:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uojQxDBYEtXF"
   },
   "outputs": [],
   "source": [
    "data = email('<|email|>', truncate=True, gpt2_type='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xUGNsc7zF9B4",
    "outputId": "6022d652-41b1-4d9f-ccb9-9acc31994194"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17402it [02:56, 98.61it/s]\n"
     ]
    }
   ],
   "source": [
    "model = train(\n",
    "    data,\n",
    "    GPT2LMHeadModel.from_pretrained(gpt2_type),\n",
    "    GPT2Tokenizer.from_pretrained(gpt2_type),\n",
    "    batch_size=16,\n",
    "    epochs=1,\n",
    "    lr=3e-5,\n",
    "    max_seq_len=140,\n",
    "    warmup_steps=5000,\n",
    "    gpt2_type=gpt2_type,\n",
    "    device=\"cuda\",\n",
    "    output_dir=\"trained_models\",\n",
    "    output_prefix=\"email\",\n",
    "    save_model_on_epoch=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HisjavqwF-pA"
   },
   "outputs": [],
   "source": [
    "# adapted from Huggingface's run_generation.py script\n",
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    entry_count=10,\n",
    "    entry_length=100,\n",
    "    top_p=0.8,\n",
    "    temperature=1.,\n",
    "):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    generated_num = 0\n",
    "    generated_list = []\n",
    "\n",
    "    filter_value = -float(\"Inf\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for entry_idx in trange(entry_count):\n",
    "\n",
    "            entry_finished = False\n",
    "\n",
    "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "\n",
    "            for i in range(entry_length):\n",
    "                outputs = model(generated, labels=generated)\n",
    "                loss, logits = outputs[:2]\n",
    "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(\n",
    "                    F.softmax(sorted_logits, dim=-1), dim=-1\n",
    "                )\n",
    "\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                    ..., :-1\n",
    "                ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = filter_value\n",
    "\n",
    "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "                generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n",
    "                    entry_finished = True\n",
    "\n",
    "                if entry_finished:\n",
    "\n",
    "                    generated_num = generated_num + 1\n",
    "\n",
    "                    output_list = list(generated.squeeze().numpy())\n",
    "                    output_text = tokenizer.decode(output_list)\n",
    "\n",
    "                    generated_list.append(output_text)\n",
    "                    break\n",
    "            \n",
    "            if not entry_finished:\n",
    "                output_list = list(generated.squeeze().numpy())\n",
    "                output_text = f\"{tokenizer.decode(output_list)}<|endoftext|>\" \n",
    "                generated_list.append(output_text)\n",
    "                \n",
    "    return generated_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "460CKMuyKjDd",
    "outputId": "6bf2f973-85e8-4bb7-87ba-50bddb6f9690"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:36<00:00, 36.60s/it]\n"
     ]
    }
   ],
   "source": [
    "generated_emails = generate(model.to('cpu'), GPT2Tokenizer.from_pretrained('gpt2'),\"Financially,\",entry_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tJe5nd4_Ksaa",
    "outputId": "1e158221-9fcf-427e-fd16-651928f35437"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Financially, I need money so I\\'ve heard about Venezuela. But what\\'s happening in Venezuela? Is there a difference in economics between North America and Venezuela?\"\\n\\nMr. Obama was a friend of Mr. Chavez and knows him well. And a close friend of President Chavez, Mr. Rouhani, who has been a friend of President Obama, was also there.\\n\\nMr. Rouhani has been pretty friendly with the regime. When we talked about his conversations, I have said before that he was very kind<|endoftext|>']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GKJ-3ReBL12X"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "capstone.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
