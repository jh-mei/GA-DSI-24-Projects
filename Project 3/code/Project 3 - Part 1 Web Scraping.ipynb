{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b641732",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3 - Part 1: Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f7524e",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bbe553",
   "metadata": {},
   "source": [
    "Reddit is a very diverse collection of forums, divided categorically into smaller forums called subreddits. The main purpose of the subreddit, however, tends to tread a very fine line. For example, /r/stocks and /r/wallstreetbets are both about stocks, but the direction the posts tend to head to are quite different.\n",
    "\n",
    "Imagine that Reddit would like to venture into the news industry. The news industry would feed them articles, and Reddit would automatically propagate them to subreddits via a bot, where they think people will be interested in clicking on them. But they also need to strike a balance so that people will not treat it as spam and thus get annoyed by their website. \n",
    "\n",
    "The final bot should be able to take in the headlines of articles and predict what kind of subreddits they should be propagated into, without using the subject of the article.\n",
    "\n",
    "This project aims to train a classifier that will serve as the basis of such a bot, by classifying what kind of subreddit a length of text (news article header) will fit in based on the contents of posts that were already on that subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819e8399",
   "metadata": {},
   "source": [
    "### Contents:\n",
    "- [Background](#Background)\n",
    "- [Web Scraping](#Web-Scraping)\n",
    "- [Preprocessing](#Preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b10535",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a043eb9e",
   "metadata": {},
   "source": [
    "The two subreddits chosen are **/r/magicTCG** and **/r/mtgfinance**. While both are related, the latter discusses the cards' financial prices while the former discusses gameplay and the metagame. What is interesting is that both subreddits tend to also contain posts you would more commonly find in the other. For example, it is possible to find finance discussions on /r/magicTCG and metagame discussions on /r/mtgfinance (the moderators try to prevent this from happening, but the system isn't perfect). \n",
    "\n",
    "If the classifier was able to classify posts between these two subreddits with a good accuracy, it should be able to distinguish between most subreddits as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b7bd6b",
   "metadata": {},
   "source": [
    "### A short intro to /r/magicTCG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb2fa7e",
   "metadata": {},
   "source": [
    "Magic: the Gathering is a Trading Card Game (TCG) introduced in 1993. It was THE original TCG, in fact, and thus naturally has a large collectible value to it. Boasting a global playerbase of 35 million, it is also the most widely played TCG in the world. For more details, refer to the Wikipedia page.\n",
    "(https://en.wikipedia.org/wiki/Magic:_The_Gathering)\n",
    "\n",
    "The subreddit dedicated to this game is /r/magicTCG. The main forms of textual posts in this subreddit are usually questions about the gameplay, like whether the 252 page rulebook is comprehensive enough, and discussions regarding the lore. Most other posts are pictorial, like fanarts and card alters.\n",
    "\n",
    "However, every quarter or so, the company behind this game, Wizards of the Coast (abbrev. Wotc), releases an expansion (aka 'set' to the playerbase). These expansions are major events in the subreddit and will take over the majority of the posts as spoilers of the new cards are released."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bd54f5",
   "metadata": {},
   "source": [
    "### A short intro to /r/mtgfinance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0d8149",
   "metadata": {},
   "source": [
    "And then we have /r/mtgfinance, the self-proclaimed /r/wallstreetbets of Magic: the Gathering with all of the same sass and attitude.\n",
    "\n",
    "The posts on that subreddit are mostly dedicated to asking whether it is wise to hold onto old sets as investments, asking whether buying a suspicious Black Lotus for $500,000 is wise (https://www.polygon.com/2021/1/27/22253079/magic-the-gathering-black-lotus-auction-price-2021) and flipping cards like they were cryptocurrencies.\n",
    "\n",
    "Two main topics that frequently turn up are 'Secret Lairs' and 'Reserved List'. \n",
    "\n",
    "'Secret Lairs' are a fairly new initiative by Wotc (who was recently acquired by Hasbro). Basically, they are a collection of reprinted old cards with new art that are then purchasable online. This initiative has garnered a lot of flak from the main subreddit with people calling the move 'greedy' as Wotc had always stayed out of manipulating the secondary market until recently. /r/mtgfinance people, however, see each Secret Lair release as a potential investment, buying them so that time will appreciate their value.\n",
    "\n",
    "'Reserved List' is a list of old cards that Wotc has declared that they will never reprint, else they suffer the legal consequences. Clearly, this causes scarcity, and scarcity leads to demand, and demand leads to high prices. This is the basis of the subreddit, and many people who use this subreddit are very interested in collecting these Reserved List cards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d729af1",
   "metadata": {},
   "source": [
    "### Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c954392",
   "metadata": {},
   "source": [
    "The following steps will be taken to complete this project.\n",
    "\n",
    "   1. Collection of data through web scraping\n",
    "   \n",
    "   2. Preprocessing to remove common words, define stop words etc.\n",
    "   \n",
    "   3. Performing of EDA to take a look at preliminary effects of lemmatization and stemming to see what kind of effect it should have on modelling\n",
    "   \n",
    "   4. Modelling and inferences\n",
    "   \n",
    "   5. Draw final conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c728bb",
   "metadata": {},
   "source": [
    "### Datasets Used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3faac49",
   "metadata": {},
   "source": [
    "* [`test.csv`](../datasets/test.csv): Kaggle dataset\n",
    "* [`train.csv`](../datasets/train.csv): Training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d01d08b",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acc2deea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# scraping libraries\n",
    "import requests\n",
    "\n",
    "# nlp libraries\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0e7e6f",
   "metadata": {},
   "source": [
    "### Functions Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f1dac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts(subreddit, n):\n",
    "    full_df = pd.DataFrame() # instantiating empty dataframe\n",
    "    last_post = None # setting last epoch to None\n",
    "    for _ in range(n): # setting up for loop\n",
    "        data = requests.get(\n",
    "            'https://api.pushshift.io/reddit/search/submission',\n",
    "            {\n",
    "                'subreddit': subreddit,\n",
    "                'size': 100,\n",
    "                'before': last_post\n",
    "            } # applying api to pull latest 100 posts\n",
    "        ).json()\n",
    "        \n",
    "        df = pd.DataFrame(data['data']) # convert 100 posts to dataframe\n",
    "        last_post = df.iloc[-1]['created_utc'] # update last epoch\n",
    "        full_df = pd.concat([full_df, df]).reset_index(drop=True) # concat each dataframe\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfa3c51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_columns(df):\n",
    "    df = df.loc[\n",
    "        :,\n",
    "        [\n",
    "            'selftext',\n",
    "            'title'\n",
    "        ]\n",
    "    ]\n",
    "    \n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "602d7e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_na(df):\n",
    "    df = df.replace( # replacing removed and deleted posts with NaN\n",
    "        [\n",
    "            '',\n",
    "            '[removed]',\n",
    "            '[deleted]'],\n",
    "        np.nan\n",
    "    ).dropna( # dropping NaNs\n",
    "    ).drop_duplicates( # dropping duplicates\n",
    "    ).reset_index( # resetting index\n",
    "        drop=True\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5422b11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_text(df):\n",
    "    df['alltext'] = df['selftext'] + ' ' + df['title']\n",
    "    df = df.drop(columns=['selftext', 'title'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87f5b527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate(row):\n",
    "    if row['selftext'] == row['title']:\n",
    "        row['title'] = ' '  \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9d15648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(df):\n",
    "    df['alltext'] = df['alltext'].apply(lambda row: re.sub(r'http\\S+', '', row))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c25d70e",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea866a45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scrape posts and convert to df\n",
    "tcg_df = get_posts('magicTCG', 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbcbb2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mtgfin_df = get_posts('mtgfinance', 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b2622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if scrape was successful\n",
    "tcg_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d9ddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtgfin_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3735da46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolating the columns with text that we actually want to analyze\n",
    "# removing [removed] posts and [deleted] posts and blanks posts\n",
    "tcg_df = get_relevant_columns(tcg_df)\n",
    "tcg_df = drop_na(tcg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982c9967",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtgfin_df = get_relevant_columns(mtgfin_df)\n",
    "mtgfin_df = drop_na(mtgfin_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb6eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if there are enough posts leftover\n",
    "tcg_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968adaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtgfin_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b97213",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88cabca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates in selftext and title\n",
    "tcg_df.loc[(tcg_df['selftext'] == tcg_df['title']), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42b5678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates in selftext and title\n",
    "mtgfin_df.loc[(mtgfin_df['selftext'] == mtgfin_df['title']), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e13a9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing duplicate text\n",
    "mtgfin_df = mtgfin_df.apply(remove_duplicate, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb51cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging selftext and title columns\n",
    "tcg_df = merge_text(tcg_df)\n",
    "mtgfin_df = merge_text(mtgfin_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475875b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove urls\n",
    "remove_url(mtgfin_df)\n",
    "remove_url(tcg_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7caa3c",
   "metadata": {},
   "source": [
    "The word 'finance' is intuitively only in the financial subreddit of the TCG, so we should remove it and any stemmed version of the word as it will make differentiating the subreddits too straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6373b0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate porterstemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "# stem the word 'finance'\n",
    "p_stemmer.stem('finance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81129759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# lemmatize the word 'finance'\n",
    "lemmatizer.lemmatize('finance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac1388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all instances of the words 'finance' and 'financ'\n",
    "tcg_df['alltext'] = tcg_df['alltext'].map(lambda x: x.replace('finance', '').replace('financ', ''))\n",
    "mtgfin_df['alltext'] = mtgfin_df['alltext'].map(lambda x: x.replace('finance', '').replace('financ', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b201c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtgfin_df['alltext'].str.contains('finance').sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5203c87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv\n",
    "tcg_df.to_csv('../datasets/tcg_df_clean.csv', index=False)\n",
    "mtgfin_df.to_csv('../datasets/mtgfin_df_clean.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
